{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:26:58.498037Z",
     "iopub.status.busy": "2022-12-13T21:26:58.497299Z",
     "iopub.status.idle": "2022-12-13T21:26:58.522662Z",
     "shell.execute_reply": "2022-12-13T21:26:58.520627Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set up logger\n",
    "import os\n",
    "import logging\n",
    "\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"1\"\n",
    "os.environ[\"AUTOGRAPH_VERBOSITY\"] = \"1\"\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.handlers = []\n",
    "ch = logging.StreamHandler()\n",
    "formatter = logging.Formatter(\n",
    "    fmt=\"%(asctime)s (%(levelname)s): %(message)s\", datefmt=\"%Y-%m-%d %H:%M:%S\"\n",
    ")\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "logger.setLevel(\"INFO\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:26:58.530577Z",
     "iopub.status.busy": "2022-12-13T21:26:58.529900Z",
     "iopub.status.idle": "2022-12-13T21:27:00.536830Z",
     "shell.execute_reply": "2022-12-13T21:27:00.536337Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import yaml\n",
    "import string\n",
    "import ast\n",
    "import random\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "from gemnet.model.gemnet import GemNet\n",
    "from gemnet.training.trainer import Trainer\n",
    "from gemnet.training.metrics import Metrics, BestMetrics\n",
    "from gemnet.training.data_container import DataContainer\n",
    "from gemnet.training.data_provider import DataProvider\n",
    "\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:00.538893Z",
     "iopub.status.busy": "2022-12-13T21:27:00.538658Z",
     "iopub.status.idle": "2022-12-13T21:27:00.545179Z",
     "shell.execute_reply": "2022-12-13T21:27:00.544750Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('config.yaml', 'r') as c:\n",
    "    config = yaml.safe_load(c)\n",
    "    \n",
    "# For strings that yaml doesn't parse (e.g. None)\n",
    "for key, val in config.items():\n",
    "    if type(val) is str:\n",
    "        try:\n",
    "            config[key] = ast.literal_eval(val)\n",
    "        except (ValueError, SyntaxError):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:00.546841Z",
     "iopub.status.busy": "2022-12-13T21:27:00.546707Z",
     "iopub.status.idle": "2022-12-13T21:27:00.552178Z",
     "shell.execute_reply": "2022-12-13T21:27:00.551749Z"
    }
   },
   "outputs": [],
   "source": [
    "num_spherical = config[\"num_spherical\"]\n",
    "num_radial = config[\"num_radial\"]\n",
    "num_blocks = config[\"num_blocks\"]\n",
    "emb_size_atom = config[\"emb_size_atom\"]\n",
    "emb_size_edge = config[\"emb_size_edge\"]\n",
    "emb_size_trip = config[\"emb_size_trip\"]\n",
    "emb_size_quad = config[\"emb_size_quad\"]\n",
    "emb_size_rbf = config[\"emb_size_rbf\"]\n",
    "emb_size_cbf = config[\"emb_size_cbf\"]\n",
    "emb_size_sbf = config[\"emb_size_sbf\"]\n",
    "num_before_skip = config[\"num_before_skip\"]\n",
    "num_after_skip = config[\"num_after_skip\"]\n",
    "num_concat = config[\"num_concat\"]\n",
    "num_atom = config[\"num_atom\"]\n",
    "emb_size_bil_quad = config[\"emb_size_bil_quad\"]\n",
    "emb_size_bil_trip = config[\"emb_size_bil_trip\"]\n",
    "triplets_only = config[\"triplets_only\"]\n",
    "forces_coupled = config[\"forces_coupled\"]\n",
    "direct_forces = config[\"direct_forces\"]\n",
    "mve = config[\"mve\"]\n",
    "cutoff = config[\"cutoff\"]\n",
    "int_cutoff = config[\"int_cutoff\"]\n",
    "envelope_exponent = config[\"envelope_exponent\"]\n",
    "extensive = config[\"extensive\"]\n",
    "output_init = config[\"output_init\"]\n",
    "scale_file = config[\"scale_file\"]\n",
    "data_seed = config[\"data_seed\"]\n",
    "dataset = config[\"dataset\"]\n",
    "val_dataset = config[\"val_dataset\"]\n",
    "num_train = config[\"num_train\"]\n",
    "num_val = config[\"num_val\"]\n",
    "logdir = config[\"logdir\"]\n",
    "loss = config[\"loss\"]\n",
    "tfseed = config[\"tfseed\"]\n",
    "num_steps = config[\"num_steps\"]\n",
    "rho_force = config[\"rho_force\"]\n",
    "ema_decay = config[\"ema_decay\"]\n",
    "weight_decay = config[\"weight_decay\"]\n",
    "grad_clip_max = config[\"grad_clip_max\"]\n",
    "agc = config[\"agc\"]\n",
    "decay_patience = config[\"decay_patience\"]\n",
    "decay_factor = config[\"decay_factor\"]\n",
    "decay_cooldown = config[\"decay_cooldown\"]\n",
    "batch_size = config[\"batch_size\"]\n",
    "evaluation_interval = config[\"evaluation_interval\"]\n",
    "patience = config[\"patience\"]\n",
    "save_interval = config[\"save_interval\"]\n",
    "learning_rate = config[\"learning_rate\"]\n",
    "warmup_steps = config[\"warmup_steps\"]\n",
    "decay_steps = config[\"decay_steps\"]\n",
    "decay_rate = config[\"decay_rate\"]\n",
    "staircase = config[\"staircase\"]\n",
    "restart = config[\"restart\"]\n",
    "comment = config[\"comment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set paths and create directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:00.553849Z",
     "iopub.status.busy": "2022-12-13T21:27:00.553717Z",
     "iopub.status.idle": "2022-12-13T21:27:00.627797Z",
     "shell.execute_reply": "2022-12-13T21:27:00.627386Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 22:27:00 (INFO): Start training\n",
      "2022-12-13 22:27:00 (INFO): Available GPUs: 2\n",
      "2022-12-13 22:27:00 (INFO): CUDA Available: True\n",
      "2022-12-13 22:27:00 (INFO): Directory: logs/20221213_222700_43Fxgy_coll_v1.2_train.npz_GemNet\n",
      "2022-12-13 22:27:00 (INFO): Create directories\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(tfseed)\n",
    "\n",
    "logging.info(\"Start training\")\n",
    "num_gpus = torch.cuda.device_count()\n",
    "cuda_available = torch.cuda.is_available()\n",
    "logging.info(f\"Available GPUs: {num_gpus}\")\n",
    "logging.info(f\"CUDA Available: {cuda_available}\")\n",
    "if num_gpus == 0:\n",
    "    logging.warning(\"No GPUs were found. Training is run on CPU!\")\n",
    "if not cuda_available:\n",
    "    logging.warning(\"CUDA unavailable. Training is run on CPU!\")\n",
    "\n",
    "# Used for creating a \"unique\" id for a run (almost impossible to generate the same twice)\n",
    "def id_generator(\n",
    "    size=6, chars=string.ascii_uppercase + string.ascii_lowercase + string.digits\n",
    "):\n",
    "    return \"\".join(random.SystemRandom().choice(chars) for _ in range(size))\n",
    "\n",
    "# A unique directory name is created for this run based on the input\n",
    "if (restart is None) or (restart == \"None\"):\n",
    "    directory = (\n",
    "        logdir\n",
    "        + \"/\"\n",
    "        + datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        + \"_\"\n",
    "        + id_generator()\n",
    "        + \"_\"\n",
    "        + os.path.basename(dataset)\n",
    "        + \"_\"\n",
    "        + str(comment)\n",
    "    )\n",
    "else:\n",
    "    directory = restart\n",
    "\n",
    "logging.info(f\"Directory: {directory}\")\n",
    "logging.info(\"Create directories\")\n",
    "\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "\n",
    "best_dir = os.path.join(directory, \"best\")\n",
    "if not os.path.exists(best_dir):\n",
    "    os.makedirs(best_dir)\n",
    "log_dir = os.path.join(directory, \"logs\")\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "extension = \".pth\"\n",
    "log_path_model = f\"{log_dir}/model{extension}\"\n",
    "log_path_training = f\"{log_dir}/training{extension}\"\n",
    "best_path_model = f\"{best_dir}/model{extension}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:00.629550Z",
     "iopub.status.busy": "2022-12-13T21:27:00.629409Z",
     "iopub.status.idle": "2022-12-13T21:27:42.159791Z",
     "shell.execute_reply": "2022-12-13T21:27:42.159332Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 22:27:00 (INFO): Initialize model\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GemNet(\n",
       "  (rbf_basis): BesselBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (cbf_basis): SphericalBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (sbf_basis): TensorBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (cbf_basis3): SphericalBasisLayer(\n",
       "    (envelope): Envelope()\n",
       "  )\n",
       "  (mlp_rbf4): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_cbf4): Dense(\n",
       "    (linear): Linear(in_features=42, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_sbf4): EfficientInteractionDownProjection()\n",
       "  (mlp_rbf3): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_cbf3): EfficientInteractionDownProjection()\n",
       "  (mlp_rbf_h): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (mlp_rbf_out): Dense(\n",
       "    (linear): Linear(in_features=6, out_features=16, bias=False)\n",
       "    (_activation): Identity()\n",
       "  )\n",
       "  (atom_emb): AtomEmbedding(\n",
       "    (embeddings): Embedding(93, 128)\n",
       "  )\n",
       "  (edge_emb): EdgeEmbedding(\n",
       "    (dense): Dense(\n",
       "      (linear): Linear(in_features=262, out_features=128, bias=False)\n",
       "      (_activation): ScaledSiLU(\n",
       "        (_activation): SiLU()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (out_blocks): ModuleList(\n",
       "    (0): OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "    (1): OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "    (2): OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "    (3): OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "    (4): OutputBlock(\n",
       "      (dense_rbf): Dense(\n",
       "        (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "      (scale_sum): ScalingFactor()\n",
       "      (layers): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (seq_energy): ModuleList(\n",
       "        (0): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (1): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (2): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (out_energy): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=1, bias=False)\n",
       "        (_activation): Identity()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (int_blocks): ModuleList(\n",
       "    (0): InteractionBlock(\n",
       "      (dense_ca): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (_activation): ScaledSiLU(\n",
       "          (_activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (quad_interaction): QuadrupletInteraction(\n",
       "        (dense_db): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_cbf): ScalingFactor()\n",
       "        (mlp_sbf): EfficientInteractionBilinear()\n",
       "        (scale_sbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (trip_interaction): TripletInteraction(\n",
       "        (dense_ba): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): EfficientInteractionBilinear()\n",
       "        (scale_cbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_before_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_after_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (atom_update): AtomUpdateBlock(\n",
       "        (dense_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_sum): ScalingFactor()\n",
       "        (layers): ModuleList(\n",
       "          (0): Dense(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (_activation): ScaledSiLU(\n",
       "              (_activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (concat_layer): EdgeEmbedding(\n",
       "        (dense): Dense(\n",
       "          (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual_m): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): InteractionBlock(\n",
       "      (dense_ca): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (_activation): ScaledSiLU(\n",
       "          (_activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (quad_interaction): QuadrupletInteraction(\n",
       "        (dense_db): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_cbf): ScalingFactor()\n",
       "        (mlp_sbf): EfficientInteractionBilinear()\n",
       "        (scale_sbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (trip_interaction): TripletInteraction(\n",
       "        (dense_ba): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): EfficientInteractionBilinear()\n",
       "        (scale_cbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_before_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_after_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (atom_update): AtomUpdateBlock(\n",
       "        (dense_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_sum): ScalingFactor()\n",
       "        (layers): ModuleList(\n",
       "          (0): Dense(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (_activation): ScaledSiLU(\n",
       "              (_activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (concat_layer): EdgeEmbedding(\n",
       "        (dense): Dense(\n",
       "          (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual_m): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): InteractionBlock(\n",
       "      (dense_ca): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (_activation): ScaledSiLU(\n",
       "          (_activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (quad_interaction): QuadrupletInteraction(\n",
       "        (dense_db): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_cbf): ScalingFactor()\n",
       "        (mlp_sbf): EfficientInteractionBilinear()\n",
       "        (scale_sbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (trip_interaction): TripletInteraction(\n",
       "        (dense_ba): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): EfficientInteractionBilinear()\n",
       "        (scale_cbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_before_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_after_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (atom_update): AtomUpdateBlock(\n",
       "        (dense_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_sum): ScalingFactor()\n",
       "        (layers): ModuleList(\n",
       "          (0): Dense(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (_activation): ScaledSiLU(\n",
       "              (_activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (concat_layer): EdgeEmbedding(\n",
       "        (dense): Dense(\n",
       "          (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual_m): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (3): InteractionBlock(\n",
       "      (dense_ca): Dense(\n",
       "        (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (_activation): ScaledSiLU(\n",
       "          (_activation): SiLU()\n",
       "        )\n",
       "      )\n",
       "      (quad_interaction): QuadrupletInteraction(\n",
       "        (dense_db): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=32, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_cbf): ScalingFactor()\n",
       "        (mlp_sbf): EfficientInteractionBilinear()\n",
       "        (scale_sbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=32, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=32, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (trip_interaction): TripletInteraction(\n",
       "        (dense_ba): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (mlp_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_rbf): ScalingFactor()\n",
       "        (mlp_cbf): EfficientInteractionBilinear()\n",
       "        (scale_cbf_sum): ScalingFactor()\n",
       "        (down_projection): Dense(\n",
       "          (linear): Linear(in_features=128, out_features=64, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ca): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "        (up_projection_ac): Dense(\n",
       "          (linear): Linear(in_features=64, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_before_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (layers_after_skip): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (atom_update): AtomUpdateBlock(\n",
       "        (dense_rbf): Dense(\n",
       "          (linear): Linear(in_features=16, out_features=128, bias=False)\n",
       "          (_activation): Identity()\n",
       "        )\n",
       "        (scale_sum): ScalingFactor()\n",
       "        (layers): ModuleList(\n",
       "          (0): Dense(\n",
       "            (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "            (_activation): ScaledSiLU(\n",
       "              (_activation): SiLU()\n",
       "            )\n",
       "          )\n",
       "          (1): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "          (2): ResidualLayer(\n",
       "            (dense_mlp): Sequential(\n",
       "              (0): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "              (1): Dense(\n",
       "                (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "                (_activation): ScaledSiLU(\n",
       "                  (_activation): SiLU()\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (concat_layer): EdgeEmbedding(\n",
       "        (dense): Dense(\n",
       "          (linear): Linear(in_features=384, out_features=128, bias=False)\n",
       "          (_activation): ScaledSiLU(\n",
       "            (_activation): SiLU()\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (residual_m): ModuleList(\n",
       "        (0): ResidualLayer(\n",
       "          (dense_mlp): Sequential(\n",
       "            (0): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "            (1): Dense(\n",
       "              (linear): Linear(in_features=128, out_features=128, bias=False)\n",
       "              (_activation): ScaledSiLU(\n",
       "                (_activation): SiLU()\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.info(\"Initialize model\")\n",
    "model = GemNet(\n",
    "    num_spherical=num_spherical,\n",
    "    num_radial=num_radial,\n",
    "    num_blocks=num_blocks,\n",
    "    emb_size_atom=emb_size_atom,\n",
    "    emb_size_edge=emb_size_edge,\n",
    "    emb_size_trip=emb_size_trip,\n",
    "    emb_size_quad=emb_size_quad,\n",
    "    emb_size_rbf=emb_size_rbf,\n",
    "    emb_size_cbf=emb_size_cbf,\n",
    "    emb_size_sbf=emb_size_sbf,\n",
    "    num_before_skip=num_before_skip,\n",
    "    num_after_skip=num_after_skip,\n",
    "    num_concat=num_concat,\n",
    "    num_atom=num_atom,\n",
    "    emb_size_bil_quad=emb_size_bil_quad,\n",
    "    emb_size_bil_trip=emb_size_bil_trip,\n",
    "    num_targets=2 if mve else 1,\n",
    "    triplets_only=triplets_only,\n",
    "    direct_forces=direct_forces,\n",
    "    forces_coupled=forces_coupled,\n",
    "    cutoff=cutoff,\n",
    "    int_cutoff=int_cutoff,\n",
    "    envelope_exponent=envelope_exponent,\n",
    "    activation=\"swish\",\n",
    "    extensive=extensive,\n",
    "    output_init=output_init,\n",
    "    scale_file=scale_file,\n",
    ")\n",
    "# push to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:42.161602Z",
     "iopub.status.busy": "2022-12-13T21:27:42.161460Z",
     "iopub.status.idle": "2022-12-13T21:27:42.238531Z",
     "shell.execute_reply": "2022-12-13T21:27:42.238171Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 22:27:42 (INFO): Load dataset\n",
      "2022-12-13 22:27:42 (INFO): Training data size: 120000\n",
      "2022-12-13 22:27:42 (INFO): Validation data size: 10000\n"
     ]
    }
   ],
   "source": [
    "train = {}\n",
    "validation = {}\n",
    "\n",
    "logging.info(\"Load dataset\")\n",
    "data_container = DataContainer(\n",
    "    dataset, cutoff=cutoff, int_cutoff=int_cutoff, triplets_only=triplets_only\n",
    ")\n",
    "\n",
    "if val_dataset is not None:\n",
    "    # Initialize DataProvider\n",
    "    if num_train == 0:\n",
    "        num_train = len(data_container)\n",
    "    logging.info(f\"Training data size: {num_train}\")\n",
    "    data_provider = DataProvider(\n",
    "        data_container,\n",
    "        num_train,\n",
    "        0,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "\n",
    "    # Initialize validation datasets\n",
    "    val_data_container = DataContainer(\n",
    "        val_dataset,\n",
    "        cutoff=cutoff,\n",
    "        int_cutoff=int_cutoff,\n",
    "        triplets_only=triplets_only,\n",
    "    )\n",
    "    if num_val == 0:\n",
    "        num_val = len(val_data_container)\n",
    "    logging.info(f\"Validation data size: {num_val}\")\n",
    "    val_data_provider = DataProvider(\n",
    "        val_data_container,\n",
    "        0,\n",
    "        num_val,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "else:\n",
    "    # Initialize DataProvider (splits dataset into 3 sets based on data_seed and provides tf.datasets)\n",
    "    logging.info(f\"Training data size: {num_train}\")\n",
    "    logging.info(f\"Validation data size: {num_val}\")\n",
    "    assert num_train > 0\n",
    "    assert num_val > 0\n",
    "    data_provider = DataProvider(\n",
    "        data_container,\n",
    "        num_train,\n",
    "        num_val,\n",
    "        batch_size,\n",
    "        seed=data_seed,\n",
    "        shuffle=True,\n",
    "        random_split=True,\n",
    "    )\n",
    "    val_data_provider = data_provider\n",
    "\n",
    "# Initialize datasets\n",
    "train[\"dataset_iter\"] = data_provider.get_dataset(\"train\")\n",
    "validation[\"dataset_iter\"] = val_data_provider.get_dataset(\"val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:42.240308Z",
     "iopub.status.busy": "2022-12-13T21:27:42.240172Z",
     "iopub.status.idle": "2022-12-13T21:27:42.249692Z",
     "shell.execute_reply": "2022-12-13T21:27:42.249368Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 22:27:42 (INFO): Prepare training\n",
      "2022-12-13 22:27:42 (INFO): Freshly initialize model\n"
     ]
    }
   ],
   "source": [
    "logging.info(\"Prepare training\")\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    learning_rate=learning_rate,\n",
    "    decay_steps=decay_steps,\n",
    "    decay_rate=decay_rate,\n",
    "    warmup_steps=warmup_steps,\n",
    "    weight_decay=weight_decay,\n",
    "    ema_decay=ema_decay,\n",
    "    decay_patience=decay_patience,\n",
    "    decay_factor=decay_factor,\n",
    "    decay_cooldown=decay_cooldown,\n",
    "    grad_clip_max=grad_clip_max,\n",
    "    rho_force=rho_force,\n",
    "    mve=mve,\n",
    "    loss=loss,\n",
    "    staircase=staircase,\n",
    "    agc=agc,\n",
    ")\n",
    "\n",
    "# Initialize metrics\n",
    "train[\"metrics\"] = Metrics(\"train\", trainer.tracked_metrics)\n",
    "validation[\"metrics\"] = Metrics(\"val\", trainer.tracked_metrics)\n",
    "\n",
    "# Save/load best recorded loss (only the best model is saved)\n",
    "metrics_best = BestMetrics(best_dir, validation[\"metrics\"])\n",
    "\n",
    "# Set up checkpointing\n",
    "# Restore latest checkpoint\n",
    "if os.path.exists(log_path_model):\n",
    "    logging.info(\"Restoring model and trainer\")\n",
    "    model_checkpoint = torch.load(log_path_model)\n",
    "    model.load_state_dict(model_checkpoint[\"model\"])\n",
    "\n",
    "    train_checkpoint = torch.load(log_path_training)\n",
    "    trainer.load_state_dict(train_checkpoint[\"trainer\"])\n",
    "    # restore the best saved results\n",
    "    metrics_best.restore()\n",
    "    logging.info(f\"Restored best metrics: {metrics_best.loss}\")\n",
    "    step_init = int(train_checkpoint[\"step\"])\n",
    "else:\n",
    "    logging.info(\"Freshly initialize model\")\n",
    "    metrics_best.inititalize()\n",
    "    step_init = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-13T21:27:42.251439Z",
     "iopub.status.busy": "2022-12-13T21:27:42.251303Z",
     "iopub.status.idle": "2022-12-18T09:16:19.113667Z",
     "shell.execute_reply": "2022-12-18T09:16:19.113175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-13 23:16:00 (INFO): 7500/3000000 (epoch 1): loss: train=0.536498, val=0.214099; energy_mae: train=3.023592, val=0.442668; force_mae: train=0.265616, val=0.105958; force_rmse: train=0.533068, val=0.213334\n",
      "2022-12-14 00:04:58 (INFO): 15000/3000000 (epoch 2): loss: train=0.232153, val=0.146097; energy_mae: train=0.484964, val=0.238206; force_mae: train=0.115323, val=0.072545; force_rmse: train=0.231406, val=0.145682\n",
      "2022-12-14 00:53:48 (INFO): 22500/3000000 (epoch 3): loss: train=0.183331, val=0.123915; energy_mae: train=0.396985, val=0.188671; force_mae: train=0.091133, val=0.061592; force_rmse: train=0.182803, val=0.123614\n",
      "2022-12-14 01:43:09 (INFO): 30000/3000000 (epoch 4): loss: train=0.158276, val=0.111045; energy_mae: train=0.323376, val=0.161576; force_mae: train=0.078721, val=0.055134; force_rmse: train=0.157888, val=0.110793\n",
      "2022-12-14 02:31:42 (INFO): 37500/3000000 (epoch 5): loss: train=0.143669, val=0.103092; energy_mae: train=0.286816, val=0.140656; force_mae: train=0.071451, val=0.051219; force_rmse: train=0.143281, val=0.102873\n",
      "2022-12-14 03:20:21 (INFO): 45000/3000000 (epoch 6): loss: train=0.131704, val=0.097524; energy_mae: train=0.271952, val=0.128344; force_mae: train=0.065505, val=0.048473; force_rmse: train=0.131342, val=0.097346\n",
      "2022-12-14 04:09:28 (INFO): 52500/3000000 (epoch 7): loss: train=0.123501, val=0.091507; energy_mae: train=0.256552, val=0.113600; force_mae: train=0.061448, val=0.045487; force_rmse: train=0.123188, val=0.091347\n",
      "2022-12-14 04:58:18 (INFO): 60000/3000000 (epoch 8): loss: train=0.115320, val=0.088467; energy_mae: train=0.240148, val=0.106147; force_mae: train=0.057363, val=0.043910; force_rmse: train=0.115015, val=0.088313\n",
      "2022-12-14 05:46:57 (INFO): 67500/3000000 (epoch 9): loss: train=0.110324, val=0.084930; energy_mae: train=0.227634, val=0.102528; force_mae: train=0.054880, val=0.042150; force_rmse: train=0.110050, val=0.084798\n",
      "2022-12-14 06:35:54 (INFO): 75000/3000000 (epoch 10): loss: train=0.104977, val=0.083817; energy_mae: train=0.214352, val=0.100021; force_mae: train=0.052241, val=0.041506; force_rmse: train=0.104765, val=0.083636\n",
      "2022-12-14 07:25:09 (INFO): 82500/3000000 (epoch 11): loss: train=0.100767, val=0.079944; energy_mae: train=0.206092, val=0.093813; force_mae: train=0.050127, val=0.039769; force_rmse: train=0.100506, val=0.079834\n",
      "2022-12-14 08:14:27 (INFO): 90000/3000000 (epoch 12): loss: train=0.097162, val=0.078387; energy_mae: train=0.197376, val=0.093049; force_mae: train=0.048321, val=0.038942; force_rmse: train=0.096898, val=0.078248\n",
      "2022-12-14 09:03:04 (INFO): 97500/3000000 (epoch 13): loss: train=0.093370, val=0.077189; energy_mae: train=0.185459, val=0.089516; force_mae: train=0.046467, val=0.038392; force_rmse: train=0.093154, val=0.077069\n",
      "2022-12-14 09:52:17 (INFO): 105000/3000000 (epoch 14): loss: train=0.091144, val=0.075584; energy_mae: train=0.190348, val=0.089997; force_mae: train=0.045331, val=0.037520; force_rmse: train=0.090899, val=0.075420\n",
      "2022-12-14 10:41:29 (INFO): 112500/3000000 (epoch 15): loss: train=0.088445, val=0.075060; energy_mae: train=0.185108, val=0.087326; force_mae: train=0.044019, val=0.037216; force_rmse: train=0.088253, val=0.074899\n",
      "2022-12-14 11:30:19 (INFO): 120000/3000000 (epoch 16): loss: train=0.085674, val=0.073343; energy_mae: train=0.173210, val=0.085453; force_mae: train=0.042655, val=0.036505; force_rmse: train=0.085512, val=0.073232\n",
      "2022-12-14 12:19:21 (INFO): 127500/3000000 (epoch 17): loss: train=0.083710, val=0.072262; energy_mae: train=0.169903, val=0.081283; force_mae: train=0.041652, val=0.035897; force_rmse: train=0.083519, val=0.072136\n",
      "2022-12-14 13:08:36 (INFO): 135000/3000000 (epoch 18): loss: train=0.082101, val=0.071429; energy_mae: train=0.169143, val=0.080805; force_mae: train=0.040849, val=0.035522; force_rmse: train=0.081897, val=0.071296\n",
      "2022-12-14 13:57:31 (INFO): 142500/3000000 (epoch 19): loss: train=0.080610, val=0.071401; energy_mae: train=0.171923, val=0.079083; force_mae: train=0.040098, val=0.035418; force_rmse: train=0.080400, val=0.071266\n",
      "2022-12-14 14:46:06 (INFO): 150000/3000000 (epoch 20): loss: train=0.078530, val=0.069934; energy_mae: train=0.160635, val=0.077514; force_mae: train=0.039087, val=0.034749; force_rmse: train=0.078353, val=0.069800\n",
      "2022-12-14 15:35:01 (INFO): 157500/3000000 (epoch 21): loss: train=0.076838, val=0.069256; energy_mae: train=0.165566, val=0.077281; force_mae: train=0.038252, val=0.034391; force_rmse: train=0.076677, val=0.069116\n",
      "2022-12-14 16:24:05 (INFO): 165000/3000000 (epoch 22): loss: train=0.075500, val=0.068831; energy_mae: train=0.167133, val=0.077280; force_mae: train=0.037570, val=0.034209; force_rmse: train=0.075316, val=0.068715\n",
      "2022-12-14 17:13:08 (INFO): 172500/3000000 (epoch 23): loss: train=0.074198, val=0.068692; energy_mae: train=0.155258, val=0.077975; force_mae: train=0.036917, val=0.034088; force_rmse: train=0.074014, val=0.068531\n",
      "2022-12-14 18:02:21 (INFO): 180000/3000000 (epoch 24): loss: train=0.073005, val=0.067678; energy_mae: train=0.155636, val=0.075340; force_mae: train=0.036319, val=0.033607; force_rmse: train=0.072827, val=0.067559\n",
      "2022-12-14 18:50:54 (INFO): 187500/3000000 (epoch 25): loss: train=0.071930, val=0.067299; energy_mae: train=0.159332, val=0.075661; force_mae: train=0.035787, val=0.033424; force_rmse: train=0.071748, val=0.067148\n",
      "2022-12-14 19:40:02 (INFO): 195000/3000000 (epoch 26): loss: train=0.070412, val=0.066365; energy_mae: train=0.146593, val=0.074961; force_mae: train=0.035050, val=0.032987; force_rmse: train=0.070284, val=0.066260\n",
      "2022-12-14 20:29:23 (INFO): 202500/3000000 (epoch 27): loss: train=0.069530, val=0.066357; energy_mae: train=0.147078, val=0.074611; force_mae: train=0.034576, val=0.032953; force_rmse: train=0.069333, val=0.066243\n",
      "2022-12-14 21:18:18 (INFO): 210000/3000000 (epoch 28): loss: train=0.068282, val=0.066446; energy_mae: train=0.141557, val=0.075099; force_mae: train=0.033982, val=0.032967; force_rmse: train=0.068120, val=0.066298\n",
      "2022-12-14 22:07:21 (INFO): 217500/3000000 (epoch 29): loss: train=0.067465, val=0.065339; energy_mae: train=0.140634, val=0.072959; force_mae: train=0.033584, val=0.032472; force_rmse: train=0.067343, val=0.065217\n",
      "2022-12-14 22:55:58 (INFO): 225000/3000000 (epoch 30): loss: train=0.066769, val=0.065039; energy_mae: train=0.141184, val=0.073269; force_mae: train=0.033235, val=0.032358; force_rmse: train=0.066625, val=0.064925\n",
      "2022-12-14 23:45:15 (INFO): 232500/3000000 (epoch 31): loss: train=0.065344, val=0.064903; energy_mae: train=0.142669, val=0.072343; force_mae: train=0.032524, val=0.032232; force_rmse: train=0.065204, val=0.064775\n",
      "2022-12-15 00:34:28 (INFO): 240000/3000000 (epoch 32): loss: train=0.064538, val=0.064497; energy_mae: train=0.134738, val=0.070890; force_mae: train=0.032127, val=0.032037; force_rmse: train=0.064408, val=0.064366\n",
      "2022-12-15 01:23:13 (INFO): 247500/3000000 (epoch 33): loss: train=0.063700, val=0.064180; energy_mae: train=0.138520, val=0.071112; force_mae: train=0.031705, val=0.031880; force_rmse: train=0.063572, val=0.064032\n",
      "2022-12-15 02:12:13 (INFO): 255000/3000000 (epoch 34): loss: train=0.062990, val=0.064844; energy_mae: train=0.136273, val=0.071133; force_mae: train=0.031355, val=0.032141; force_rmse: train=0.062856, val=0.064725\n",
      "2022-12-15 03:01:07 (INFO): 262500/3000000 (epoch 35): loss: train=0.062282, val=0.063477; energy_mae: train=0.138140, val=0.070105; force_mae: train=0.031021, val=0.031540; force_rmse: train=0.062173, val=0.063355\n",
      "2022-12-15 03:50:11 (INFO): 270000/3000000 (epoch 36): loss: train=0.061578, val=0.063373; energy_mae: train=0.134789, val=0.069153; force_mae: train=0.030665, val=0.031500; force_rmse: train=0.061466, val=0.063265\n",
      "2022-12-15 04:39:18 (INFO): 277500/3000000 (epoch 37): loss: train=0.060829, val=0.063208; energy_mae: train=0.136648, val=0.071843; force_mae: train=0.030269, val=0.031405; force_rmse: train=0.060687, val=0.063079\n",
      "2022-12-15 05:28:37 (INFO): 285000/3000000 (epoch 38): loss: train=0.060332, val=0.062868; energy_mae: train=0.128380, val=0.070401; force_mae: train=0.030044, val=0.031270; force_rmse: train=0.060221, val=0.062753\n",
      "2022-12-15 06:17:15 (INFO): 292500/3000000 (epoch 39): loss: train=0.059435, val=0.062756; energy_mae: train=0.130078, val=0.068534; force_mae: train=0.029576, val=0.031228; force_rmse: train=0.059301, val=0.062667\n",
      "2022-12-15 07:06:20 (INFO): 300000/3000000 (epoch 40): loss: train=0.058997, val=0.062430; energy_mae: train=0.129004, val=0.068218; force_mae: train=0.029366, val=0.031030; force_rmse: train=0.058876, val=0.062327\n",
      "2022-12-15 07:55:03 (INFO): 307500/3000000 (epoch 41): loss: train=0.058223, val=0.062564; energy_mae: train=0.124721, val=0.068310; force_mae: train=0.028980, val=0.031055; force_rmse: train=0.058101, val=0.062432\n",
      "2022-12-15 08:43:42 (INFO): 315000/3000000 (epoch 42): loss: train=0.057488, val=0.062291; energy_mae: train=0.123198, val=0.067842; force_mae: train=0.028609, val=0.030927; force_rmse: train=0.057365, val=0.062174\n",
      "2022-12-15 09:32:25 (INFO): 322500/3000000 (epoch 43): loss: train=0.057365, val=0.062068; energy_mae: train=0.122894, val=0.067013; force_mae: train=0.028556, val=0.030825; force_rmse: train=0.057240, val=0.061960\n",
      "2022-12-15 10:21:23 (INFO): 330000/3000000 (epoch 44): loss: train=0.056642, val=0.062201; energy_mae: train=0.123259, val=0.066045; force_mae: train=0.028192, val=0.030887; force_rmse: train=0.056509, val=0.062115\n",
      "2022-12-15 11:10:33 (INFO): 337500/3000000 (epoch 45): loss: train=0.055993, val=0.061758; energy_mae: train=0.122373, val=0.068573; force_mae: train=0.027857, val=0.030683; force_rmse: train=0.055845, val=0.061639\n",
      "2022-12-15 11:59:56 (INFO): 345000/3000000 (epoch 46): loss: train=0.055334, val=0.061341; energy_mae: train=0.119193, val=0.066531; force_mae: train=0.027540, val=0.030509; force_rmse: train=0.055214, val=0.061231\n",
      "2022-12-15 12:49:06 (INFO): 352500/3000000 (epoch 47): loss: train=0.055029, val=0.061306; energy_mae: train=0.119707, val=0.066009; force_mae: train=0.027393, val=0.030456; force_rmse: train=0.054909, val=0.061203\n",
      "2022-12-15 13:38:01 (INFO): 360000/3000000 (epoch 48): loss: train=0.054176, val=0.061275; energy_mae: train=0.118710, val=0.065810; force_mae: train=0.026974, val=0.030442; force_rmse: train=0.054074, val=0.061158\n",
      "2022-12-15 14:26:36 (INFO): 367500/3000000 (epoch 49): loss: train=0.053725, val=0.060891; energy_mae: train=0.115971, val=0.066431; force_mae: train=0.026733, val=0.030251; force_rmse: train=0.053594, val=0.060766\n",
      "2022-12-15 15:15:59 (INFO): 375000/3000000 (epoch 50): loss: train=0.053508, val=0.060999; energy_mae: train=0.117033, val=0.065606; force_mae: train=0.026634, val=0.030282; force_rmse: train=0.053397, val=0.060896\n",
      "2022-12-15 16:05:34 (INFO): 382500/3000000 (epoch 51): loss: train=0.053023, val=0.060706; energy_mae: train=0.113669, val=0.065872; force_mae: train=0.026387, val=0.030150; force_rmse: train=0.052901, val=0.060585\n",
      "2022-12-15 16:54:34 (INFO): 390000/3000000 (epoch 52): loss: train=0.052416, val=0.060537; energy_mae: train=0.113120, val=0.066279; force_mae: train=0.026106, val=0.030106; force_rmse: train=0.052327, val=0.060439\n",
      "2022-12-15 17:43:34 (INFO): 397500/3000000 (epoch 53): loss: train=0.051929, val=0.060468; energy_mae: train=0.115741, val=0.065507; force_mae: train=0.025844, val=0.030059; force_rmse: train=0.051815, val=0.060359\n",
      "2022-12-15 18:32:23 (INFO): 405000/3000000 (epoch 54): loss: train=0.051301, val=0.060359; energy_mae: train=0.113410, val=0.065890; force_mae: train=0.025542, val=0.029990; force_rmse: train=0.051203, val=0.060247\n",
      "2022-12-15 19:21:33 (INFO): 412500/3000000 (epoch 55): loss: train=0.051052, val=0.060175; energy_mae: train=0.114446, val=0.065156; force_mae: train=0.025411, val=0.029890; force_rmse: train=0.050936, val=0.060060\n",
      "2022-12-15 20:11:01 (INFO): 420000/3000000 (epoch 56): loss: train=0.050714, val=0.060154; energy_mae: train=0.111766, val=0.065364; force_mae: train=0.025245, val=0.029912; force_rmse: train=0.050602, val=0.060054\n",
      "2022-12-15 20:59:48 (INFO): 427500/3000000 (epoch 57): loss: train=0.050262, val=0.060142; energy_mae: train=0.113408, val=0.063494; force_mae: train=0.025024, val=0.029897; force_rmse: train=0.050169, val=0.060049\n",
      "2022-12-15 21:48:49 (INFO): 435000/3000000 (epoch 58): loss: train=0.049712, val=0.059947; energy_mae: train=0.112034, val=0.064020; force_mae: train=0.024740, val=0.029784; force_rmse: train=0.049600, val=0.059850\n",
      "2022-12-15 22:38:16 (INFO): 442500/3000000 (epoch 59): loss: train=0.049290, val=0.060104; energy_mae: train=0.110416, val=0.064487; force_mae: train=0.024544, val=0.029831; force_rmse: train=0.049198, val=0.059986\n",
      "2022-12-15 23:27:42 (INFO): 450000/3000000 (epoch 60): loss: train=0.049015, val=0.059633; energy_mae: train=0.109378, val=0.064006; force_mae: train=0.024409, val=0.029653; force_rmse: train=0.048920, val=0.059536\n",
      "2022-12-16 00:17:09 (INFO): 457500/3000000 (epoch 61): loss: train=0.048546, val=0.059684; energy_mae: train=0.107997, val=0.064938; force_mae: train=0.024160, val=0.029680; force_rmse: train=0.048431, val=0.059589\n",
      "2022-12-16 01:06:14 (INFO): 465000/3000000 (epoch 62): loss: train=0.048467, val=0.059798; energy_mae: train=0.107008, val=0.063660; force_mae: train=0.024133, val=0.029701; force_rmse: train=0.048359, val=0.059699\n",
      "2022-12-16 01:55:02 (INFO): 472500/3000000 (epoch 63): loss: train=0.047893, val=0.059274; energy_mae: train=0.109445, val=0.062153; force_mae: train=0.023844, val=0.029467; force_rmse: train=0.047786, val=0.059160\n",
      "2022-12-16 02:43:43 (INFO): 480000/3000000 (epoch 64): loss: train=0.047239, val=0.059333; energy_mae: train=0.107246, val=0.062517; force_mae: train=0.023517, val=0.029471; force_rmse: train=0.047137, val=0.059215\n",
      "2022-12-16 03:32:29 (INFO): 487500/3000000 (epoch 65): loss: train=0.047063, val=0.059053; energy_mae: train=0.103558, val=0.064341; force_mae: train=0.023437, val=0.029355; force_rmse: train=0.046975, val=0.058943\n",
      "2022-12-16 04:21:59 (INFO): 495000/3000000 (epoch 66): loss: train=0.046837, val=0.059072; energy_mae: train=0.104453, val=0.062719; force_mae: train=0.023339, val=0.029374; force_rmse: train=0.046770, val=0.058976\n",
      "2022-12-16 05:11:31 (INFO): 502500/3000000 (epoch 67): loss: train=0.046225, val=0.058786; energy_mae: train=0.101457, val=0.062182; force_mae: train=0.023017, val=0.029230; force_rmse: train=0.046128, val=0.058694\n",
      "2022-12-16 06:00:39 (INFO): 510000/3000000 (epoch 68): loss: train=0.046016, val=0.058840; energy_mae: train=0.101716, val=0.063392; force_mae: train=0.022918, val=0.029263; force_rmse: train=0.045940, val=0.058760\n",
      "2022-12-16 06:49:54 (INFO): 517500/3000000 (epoch 69): loss: train=0.045617, val=0.058854; energy_mae: train=0.103192, val=0.061327; force_mae: train=0.022715, val=0.029241; force_rmse: train=0.045531, val=0.058756\n",
      "2022-12-16 07:39:10 (INFO): 525000/3000000 (epoch 70): loss: train=0.045627, val=0.058863; energy_mae: train=0.102051, val=0.062462; force_mae: train=0.022715, val=0.029254; force_rmse: train=0.045543, val=0.058769\n",
      "2022-12-16 08:28:21 (INFO): 532500/3000000 (epoch 71): loss: train=0.045087, val=0.058673; energy_mae: train=0.099968, val=0.062971; force_mae: train=0.022446, val=0.029156; force_rmse: train=0.044991, val=0.058567\n",
      "2022-12-16 09:17:34 (INFO): 540000/3000000 (epoch 72): loss: train=0.044885, val=0.058737; energy_mae: train=0.099065, val=0.062161; force_mae: train=0.022345, val=0.029165; force_rmse: train=0.044791, val=0.058644\n",
      "2022-12-16 10:06:16 (INFO): 547500/3000000 (epoch 73): loss: train=0.044386, val=0.058377; energy_mae: train=0.100943, val=0.061695; force_mae: train=0.022100, val=0.029028; force_rmse: train=0.044296, val=0.058287\n",
      "2022-12-16 10:55:29 (INFO): 555000/3000000 (epoch 74): loss: train=0.044146, val=0.058668; energy_mae: train=0.099446, val=0.061035; force_mae: train=0.021979, val=0.029139; force_rmse: train=0.044066, val=0.058571\n",
      "2022-12-16 11:44:04 (INFO): 562500/3000000 (epoch 75): loss: train=0.043945, val=0.058471; energy_mae: train=0.100958, val=0.062102; force_mae: train=0.021890, val=0.029048; force_rmse: train=0.043872, val=0.058374\n",
      "2022-12-16 12:33:31 (INFO): 570000/3000000 (epoch 76): loss: train=0.043604, val=0.058383; energy_mae: train=0.098045, val=0.061396; force_mae: train=0.021719, val=0.029028; force_rmse: train=0.043531, val=0.058277\n",
      "2022-12-16 13:22:48 (INFO): 577500/3000000 (epoch 77): loss: train=0.043287, val=0.058120; energy_mae: train=0.097605, val=0.060489; force_mae: train=0.021551, val=0.028911; force_rmse: train=0.043200, val=0.058032\n",
      "2022-12-16 14:11:18 (INFO): 585000/3000000 (epoch 78): loss: train=0.042965, val=0.058255; energy_mae: train=0.096746, val=0.061405; force_mae: train=0.021402, val=0.028977; force_rmse: train=0.042896, val=0.058174\n",
      "2022-12-16 15:00:16 (INFO): 592500/3000000 (epoch 79): loss: train=0.042804, val=0.058128; energy_mae: train=0.098920, val=0.060094; force_mae: train=0.021321, val=0.028906; force_rmse: train=0.042733, val=0.058043\n",
      "2022-12-16 15:49:36 (INFO): 600000/3000000 (epoch 80): loss: train=0.042437, val=0.058021; energy_mae: train=0.093347, val=0.061078; force_mae: train=0.021126, val=0.028852; force_rmse: train=0.042344, val=0.057925\n",
      "2022-12-16 16:38:34 (INFO): 607500/3000000 (epoch 81): loss: train=0.042160, val=0.058005; energy_mae: train=0.096936, val=0.060848; force_mae: train=0.020991, val=0.028837; force_rmse: train=0.042079, val=0.057919\n",
      "2022-12-16 17:27:29 (INFO): 615000/3000000 (epoch 82): loss: train=0.041922, val=0.058421; energy_mae: train=0.096285, val=0.061663; force_mae: train=0.020874, val=0.028994; force_rmse: train=0.041840, val=0.058321\n",
      "2022-12-16 18:16:12 (INFO): 622500/3000000 (epoch 83): loss: train=0.041451, val=0.057784; energy_mae: train=0.093088, val=0.060470; force_mae: train=0.020640, val=0.028733; force_rmse: train=0.041373, val=0.057701\n",
      "2022-12-16 19:05:09 (INFO): 630000/3000000 (epoch 84): loss: train=0.041333, val=0.057773; energy_mae: train=0.093135, val=0.060244; force_mae: train=0.020576, val=0.028714; force_rmse: train=0.041249, val=0.057679\n",
      "2022-12-16 19:54:19 (INFO): 637500/3000000 (epoch 85): loss: train=0.041219, val=0.057781; energy_mae: train=0.096992, val=0.060552; force_mae: train=0.020520, val=0.028740; force_rmse: train=0.041133, val=0.057705\n",
      "2022-12-16 20:43:47 (INFO): 645000/3000000 (epoch 86): loss: train=0.040766, val=0.057874; energy_mae: train=0.093352, val=0.060336; force_mae: train=0.020295, val=0.028758; force_rmse: train=0.040677, val=0.057791\n",
      "2022-12-16 21:33:17 (INFO): 652500/3000000 (epoch 87): loss: train=0.040530, val=0.057886; energy_mae: train=0.092169, val=0.059975; force_mae: train=0.020174, val=0.028743; force_rmse: train=0.040439, val=0.057783\n",
      "2022-12-16 22:22:37 (INFO): 660000/3000000 (epoch 88): loss: train=0.040257, val=0.057681; energy_mae: train=0.090795, val=0.060563; force_mae: train=0.020050, val=0.028671; force_rmse: train=0.040183, val=0.057594\n",
      "2022-12-16 23:11:55 (INFO): 667500/3000000 (epoch 89): loss: train=0.039929, val=0.057756; energy_mae: train=0.091095, val=0.060561; force_mae: train=0.019887, val=0.028697; force_rmse: train=0.039857, val=0.057669\n",
      "2022-12-17 00:01:04 (INFO): 675000/3000000 (epoch 90): loss: train=0.039923, val=0.057622; energy_mae: train=0.092516, val=0.059159; force_mae: train=0.019881, val=0.028646; force_rmse: train=0.039844, val=0.057541\n",
      "2022-12-17 00:50:06 (INFO): 682500/3000000 (epoch 91): loss: train=0.039513, val=0.057542; energy_mae: train=0.089699, val=0.058934; force_mae: train=0.019687, val=0.028596; force_rmse: train=0.039454, val=0.057458\n",
      "2022-12-17 01:38:52 (INFO): 690000/3000000 (epoch 92): loss: train=0.039212, val=0.057634; energy_mae: train=0.088345, val=0.059494; force_mae: train=0.019522, val=0.028636; force_rmse: train=0.039129, val=0.057559\n",
      "2022-12-17 02:27:55 (INFO): 697500/3000000 (epoch 93): loss: train=0.039053, val=0.057307; energy_mae: train=0.088191, val=0.058657; force_mae: train=0.019450, val=0.028519; force_rmse: train=0.038983, val=0.057239\n",
      "2022-12-17 03:16:49 (INFO): 705000/3000000 (epoch 94): loss: train=0.038835, val=0.057323; energy_mae: train=0.089533, val=0.059519; force_mae: train=0.019344, val=0.028517; force_rmse: train=0.038766, val=0.057241\n",
      "2022-12-17 04:06:05 (INFO): 712500/3000000 (epoch 95): loss: train=0.038545, val=0.057362; energy_mae: train=0.087121, val=0.060019; force_mae: train=0.019196, val=0.028520; force_rmse: train=0.038471, val=0.057277\n",
      "2022-12-17 04:54:51 (INFO): 720000/3000000 (epoch 96): loss: train=0.038324, val=0.057401; energy_mae: train=0.087805, val=0.059917; force_mae: train=0.019093, val=0.028524; force_rmse: train=0.038270, val=0.057304\n",
      "2022-12-17 05:43:29 (INFO): 727500/3000000 (epoch 97): loss: train=0.038207, val=0.057265; energy_mae: train=0.087441, val=0.059651; force_mae: train=0.019029, val=0.028460; force_rmse: train=0.038141, val=0.057171\n",
      "2022-12-17 06:32:38 (INFO): 735000/3000000 (epoch 98): loss: train=0.038041, val=0.057165; energy_mae: train=0.086892, val=0.059065; force_mae: train=0.018942, val=0.028434; force_rmse: train=0.037968, val=0.057081\n",
      "2022-12-17 07:21:23 (INFO): 742500/3000000 (epoch 99): loss: train=0.037615, val=0.057285; energy_mae: train=0.084837, val=0.059179; force_mae: train=0.018742, val=0.028505; force_rmse: train=0.037561, val=0.057216\n",
      "2022-12-17 08:10:10 (INFO): 750000/3000000 (epoch 100): loss: train=0.037540, val=0.057102; energy_mae: train=0.085997, val=0.058413; force_mae: train=0.018700, val=0.028391; force_rmse: train=0.037478, val=0.057024\n",
      "2022-12-17 08:59:27 (INFO): 757500/3000000 (epoch 101): loss: train=0.037290, val=0.057140; energy_mae: train=0.087332, val=0.059180; force_mae: train=0.018580, val=0.028417; force_rmse: train=0.037228, val=0.057058\n",
      "2022-12-17 09:48:33 (INFO): 765000/3000000 (epoch 102): loss: train=0.037099, val=0.057033; energy_mae: train=0.082776, val=0.058669; force_mae: train=0.018479, val=0.028361; force_rmse: train=0.037036, val=0.056943\n",
      "2022-12-17 10:37:19 (INFO): 772500/3000000 (epoch 103): loss: train=0.036989, val=0.057180; energy_mae: train=0.085671, val=0.058234; force_mae: train=0.018417, val=0.028424; force_rmse: train=0.036922, val=0.057097\n",
      "2022-12-17 11:26:35 (INFO): 780000/3000000 (epoch 104): loss: train=0.036728, val=0.056952; energy_mae: train=0.085568, val=0.058166; force_mae: train=0.018294, val=0.028338; force_rmse: train=0.036665, val=0.056872\n",
      "2022-12-17 12:15:46 (INFO): 787500/3000000 (epoch 105): loss: train=0.036540, val=0.056915; energy_mae: train=0.083350, val=0.058138; force_mae: train=0.018200, val=0.028318; force_rmse: train=0.036472, val=0.056827\n",
      "2022-12-17 13:04:46 (INFO): 795000/3000000 (epoch 106): loss: train=0.036316, val=0.057008; energy_mae: train=0.083231, val=0.058286; force_mae: train=0.018093, val=0.028335; force_rmse: train=0.036263, val=0.056922\n",
      "2022-12-17 13:53:38 (INFO): 802500/3000000 (epoch 107): loss: train=0.036103, val=0.057013; energy_mae: train=0.082759, val=0.057889; force_mae: train=0.017988, val=0.028331; force_rmse: train=0.036047, val=0.056931\n",
      "2022-12-17 14:42:17 (INFO): 810000/3000000 (epoch 108): loss: train=0.035932, val=0.056894; energy_mae: train=0.084108, val=0.057807; force_mae: train=0.017899, val=0.028275; force_rmse: train=0.035880, val=0.056813\n",
      "2022-12-17 15:30:53 (INFO): 817500/3000000 (epoch 109): loss: train=0.035692, val=0.056906; energy_mae: train=0.082305, val=0.058065; force_mae: train=0.017783, val=0.028299; force_rmse: train=0.035637, val=0.056819\n",
      "2022-12-17 16:19:49 (INFO): 825000/3000000 (epoch 110): loss: train=0.035496, val=0.056738; energy_mae: train=0.080621, val=0.057624; force_mae: train=0.017686, val=0.028218; force_rmse: train=0.035442, val=0.056652\n",
      "2022-12-17 17:08:43 (INFO): 832500/3000000 (epoch 111): loss: train=0.035304, val=0.056890; energy_mae: train=0.082821, val=0.057874; force_mae: train=0.017582, val=0.028277; force_rmse: train=0.035244, val=0.056799\n",
      "2022-12-17 17:57:40 (INFO): 840000/3000000 (epoch 112): loss: train=0.035175, val=0.056796; energy_mae: train=0.082940, val=0.057599; force_mae: train=0.017529, val=0.028223; force_rmse: train=0.035127, val=0.056716\n",
      "2022-12-17 18:46:41 (INFO): 847500/3000000 (epoch 113): loss: train=0.034998, val=0.056665; energy_mae: train=0.083347, val=0.058366; force_mae: train=0.017428, val=0.028186; force_rmse: train=0.034929, val=0.056585\n",
      "2022-12-17 19:35:37 (INFO): 855000/3000000 (epoch 114): loss: train=0.034750, val=0.056660; energy_mae: train=0.081368, val=0.058389; force_mae: train=0.017310, val=0.028179; force_rmse: train=0.034679, val=0.056579\n",
      "2022-12-17 20:24:15 (INFO): 862500/3000000 (epoch 115): loss: train=0.034471, val=0.056999; energy_mae: train=0.078774, val=0.057469; force_mae: train=0.017168, val=0.028337; force_rmse: train=0.034413, val=0.056917\n",
      "2022-12-17 21:13:03 (INFO): 870000/3000000 (epoch 116): loss: train=0.034374, val=0.056750; energy_mae: train=0.081766, val=0.057244; force_mae: train=0.017120, val=0.028211; force_rmse: train=0.034311, val=0.056677\n",
      "2022-12-17 22:01:55 (INFO): 877500/3000000 (epoch 117): loss: train=0.034222, val=0.056619; energy_mae: train=0.078638, val=0.057434; force_mae: train=0.017051, val=0.028146; force_rmse: train=0.034169, val=0.056537\n",
      "2022-12-17 22:50:59 (INFO): 885000/3000000 (epoch 118): loss: train=0.033958, val=0.056635; energy_mae: train=0.078806, val=0.057505; force_mae: train=0.016921, val=0.028177; force_rmse: train=0.033909, val=0.056559\n",
      "2022-12-17 23:39:30 (INFO): 892500/3000000 (epoch 119): loss: train=0.033875, val=0.056675; energy_mae: train=0.078143, val=0.057184; force_mae: train=0.016882, val=0.028169; force_rmse: train=0.033833, val=0.056580\n",
      "2022-12-18 00:28:47 (INFO): 900000/3000000 (epoch 120): loss: train=0.033602, val=0.056676; energy_mae: train=0.077694, val=0.056330; force_mae: train=0.016741, val=0.028181; force_rmse: train=0.033552, val=0.056585\n",
      "2022-12-18 01:17:48 (INFO): 907500/3000000 (epoch 121): loss: train=0.033475, val=0.056652; energy_mae: train=0.078266, val=0.057403; force_mae: train=0.016678, val=0.028177; force_rmse: train=0.033424, val=0.056566\n",
      "2022-12-18 02:06:27 (INFO): 915000/3000000 (epoch 122): loss: train=0.033195, val=0.056680; energy_mae: train=0.078311, val=0.057332; force_mae: train=0.016541, val=0.028196; force_rmse: train=0.033143, val=0.056598\n",
      "2022-12-18 02:55:31 (INFO): 922500/3000000 (epoch 123): loss: train=0.033260, val=0.056501; energy_mae: train=0.075957, val=0.057073; force_mae: train=0.016567, val=0.028093; force_rmse: train=0.033203, val=0.056418\n",
      "2022-12-18 03:44:21 (INFO): 930000/3000000 (epoch 124): loss: train=0.032915, val=0.056714; energy_mae: train=0.076249, val=0.057124; force_mae: train=0.016396, val=0.028179; force_rmse: train=0.032864, val=0.056627\n",
      "2022-12-18 04:33:08 (INFO): 937500/3000000 (epoch 125): loss: train=0.032777, val=0.056540; energy_mae: train=0.075429, val=0.056633; force_mae: train=0.016326, val=0.028105; force_rmse: train=0.032725, val=0.056454\n",
      "2022-12-18 05:22:02 (INFO): 945000/3000000 (epoch 126): loss: train=0.032665, val=0.056333; energy_mae: train=0.075260, val=0.057076; force_mae: train=0.016274, val=0.028015; force_rmse: train=0.032614, val=0.056259\n",
      "2022-12-18 06:11:02 (INFO): 952500/3000000 (epoch 127): loss: train=0.032488, val=0.056487; energy_mae: train=0.076257, val=0.057109; force_mae: train=0.016185, val=0.028094; force_rmse: train=0.032442, val=0.056411\n",
      "2022-12-18 07:00:14 (INFO): 960000/3000000 (epoch 128): loss: train=0.032412, val=0.056608; energy_mae: train=0.076613, val=0.056343; force_mae: train=0.016149, val=0.028129; force_rmse: train=0.032362, val=0.056526\n",
      "2022-12-18 07:49:38 (INFO): 967500/3000000 (epoch 129): loss: train=0.032248, val=0.056449; energy_mae: train=0.075797, val=0.056350; force_mae: train=0.016069, val=0.028078; force_rmse: train=0.032199, val=0.056361\n",
      "2022-12-18 08:38:52 (INFO): 975000/3000000 (epoch 130): loss: train=0.032043, val=0.056407; energy_mae: train=0.075824, val=0.056390; force_mae: train=0.015974, val=0.028051; force_rmse: train=0.032006, val=0.056317\n",
      "2022-12-18 09:27:38 (INFO): 982500/3000000 (epoch 131): loss: train=0.031819, val=0.056337; energy_mae: train=0.075739, val=0.056011; force_mae: train=0.015861, val=0.028027; force_rmse: train=0.031778, val=0.056258\n",
      "2022-12-18 10:16:19 (INFO): 990000/3000000 (epoch 132): loss: train=0.031822, val=0.056339; energy_mae: train=0.073518, val=0.056153; force_mae: train=0.015858, val=0.028020; force_rmse: train=0.031781, val=0.056248\n",
      "2022-12-18 10:16:19 (INFO): Step 132: reducing on plateu by 0.5.\n"
     ]
    }
   ],
   "source": [
    "summary_writer = SummaryWriter(log_dir)\n",
    "steps_per_epoch = int(np.ceil(num_train / batch_size))\n",
    "\n",
    "for step in range(step_init + 1, num_steps + 1):\n",
    "\n",
    "    # keep track of the learning rate\n",
    "    if step % 10 == 0:\n",
    "        lr = trainer.schedulers[0].get_last_lr()[0]\n",
    "        summary_writer.add_scalar(\"lr\", lr, global_step=step)\n",
    "\n",
    "    # Perform training step\n",
    "    trainer.train_on_batch(train[\"dataset_iter\"], train[\"metrics\"])\n",
    "\n",
    "    # Save progress\n",
    "    if step % save_interval == 0:\n",
    "        torch.save({\"model\": model.state_dict()}, log_path_model)\n",
    "        torch.save(\n",
    "            {\"trainer\": trainer.state_dict(), \"step\": step}, log_path_training\n",
    "        )\n",
    "\n",
    "    # Check performance on the validation set\n",
    "    if step % evaluation_interval == 0:\n",
    "\n",
    "        # Save backup variables and load averaged variables\n",
    "        trainer.save_variable_backups()\n",
    "        trainer.load_averaged_variables()\n",
    "\n",
    "        # Compute averages\n",
    "        for i in range(int(np.ceil(num_val / batch_size))):\n",
    "            trainer.test_on_batch(validation[\"dataset_iter\"], validation[\"metrics\"])\n",
    "\n",
    "        # Update and save best result\n",
    "        if validation[\"metrics\"].loss < metrics_best.loss:\n",
    "            metrics_best.update(step, validation[\"metrics\"])\n",
    "            torch.save(model.state_dict(), best_path_model)\n",
    "\n",
    "        # write to summary writer\n",
    "        metrics_best.write(summary_writer, step)\n",
    "\n",
    "        epoch = step // steps_per_epoch\n",
    "        train_metrics_res = train[\"metrics\"].result(append_tag=False)\n",
    "        val_metrics_res = validation[\"metrics\"].result(append_tag=False)\n",
    "        metrics_strings = [\n",
    "            f\"{key}: train={train_metrics_res[key]:.6f}, val={val_metrics_res[key]:.6f}\"\n",
    "            for key in validation[\"metrics\"].keys\n",
    "        ]\n",
    "        logging.info(\n",
    "            f\"{step}/{num_steps} (epoch {epoch}): \" + \"; \".join(metrics_strings)\n",
    "        )\n",
    "\n",
    "        # decay learning rate on plateau\n",
    "        trainer.decay_maybe(validation[\"metrics\"].loss)\n",
    "\n",
    "        train[\"metrics\"].write(summary_writer, step)\n",
    "        validation[\"metrics\"].write(summary_writer, step)\n",
    "        train[\"metrics\"].reset_states()\n",
    "        validation[\"metrics\"].reset_states()\n",
    "\n",
    "        # Restore backup variables\n",
    "        trainer.restore_variable_backups()\n",
    "\n",
    "        # early stopping\n",
    "        if step - metrics_best.step > patience * evaluation_interval:\n",
    "            break\n",
    "\n",
    "result = {key + \"_best\": val for key, val in metrics_best.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Print results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-18T09:16:19.115471Z",
     "iopub.status.busy": "2022-12-18T09:16:19.115332Z",
     "iopub.status.idle": "2022-12-18T09:16:19.117864Z",
     "shell.execute_reply": "2022-12-18T09:16:19.117428Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss_val: 0.05633348971605301\n",
      "energy_mae_val: 0.057075634598731995\n",
      "force_mae_val: 0.028015445917844772\n",
      "force_rmse_val: 0.05625857412815094\n",
      "step: 945000\n"
     ]
    }
   ],
   "source": [
    "for key, val in metrics_best.items():\n",
    "    print(f\"{key}: {val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gemnet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d62da17299be9defd8dbe3e03235aa2caefd9a4a0f498723db5c7f5e0b666b8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
